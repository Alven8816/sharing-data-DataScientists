---
title: "Logistic回归详解"
author: "余文华"
date: "2016年5月8日"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 前言

    Logistic回归应该是大家再熟悉不过的针对二分类或多分类因变量的回归模型之一。由于其概率表达式的显性特点，模型的可解释性强，是社会学、生物统计学、临床、数量心理学、市场营销等统计实证分析的常用方法。但是在模型的运用方面，由于不理解模型的应用特点，出现非常多的“生搬硬套”嫌疑。
    为了更深入学习Logistic回归，我将从常规Logistic回归、确切Logistic回归（Exact Logistic Regression）、多分类Logistic回归、有序Logistic回归及Probit 回归这些回归模型进行简要的介绍及R实现。由于涉及内容较多，自身也在学习当中，就学到哪写到哪，分几期来完成它。这一期让我们来看看常规的Logistic回归及R实现。
    
## Logistic回归概述

    Logistic回归最常用的是解决二分类问题，即预测或判别y=0或1。这就期望我们能构建一个分类器h(x)在0-1之间，于是就有了Logistic函数或Logistic曲线，又叫 sigmoid曲线（S型曲线）。我们尽可能的少用公式展示，一方面是不好编辑，另一方面不希望大家有望而生畏的错觉。
    由上图可以看到，当z<0时，h(x)<0.5;z>= 0时，h(x)>=0.5,且h(x)在0-1 之间 ，很好的构建了一个符合我们要求的二分类器。为更好的理解Logistic回归，我们再引入一个例子：
    研究者希望通过查看学生的GRE成绩，平均绩点GPA及毕业学校的威望排名来评估是否录取该学生，即因变量y是录取（y=1）或不录取（y=0），二分类问题。
    
```{r}
#读入数据
mydata <- read.csv("http://www.ats.ucla.edu/stat/data/binary.csv")
head(mydata)
```

    我们的目的是通过已知某个学生的GRE成绩、GPA及毕业学校来预测该生被录取的概率，也就是h(x)为纳入X自变量后，y=1时的概率值。当这个概率值h(x)<0.5时，认为不会被录取（y=0），反之则被录取。总之一点，我们求得的h(x)是y=1的概率值。

##Logistic回归模型的适用条件
    
    这部分是百度上的，适用条件其实和线性回归模型差不多，应该不难理解。
    
1. 因变量y为二分类的分类变量或某事件的发生率

2. 残差和因变量都要服从二项分布,运用最大似然法来解决方程估计和检验问题。（至于什么是二项分布，什么是最大似然法，看不懂的可以略过）

3. 自变量和Logistic概率是线性关系

4. 各观测对象间相互独立

##模型实现

    运用广义线性模型glm实现Logistic回归.
    
```{r}
library(aod)
library(ggplot2)
library(Rcpp)
mydata$rank <- factor(mydata$rank)
mylogit <- glm(admit ~ gre + gpa + rank, data = mydata, family = "binomial")
summary(mylogit)
#系数95%CI
confint(mylogit)
## odds ratios（OR值）及95%CI
exp(cbind(OR = coef(mylogit), confint(mylogit)))
```

    模型的结果给出了模型的结构，Deviance Residuals、beta系数 及P值和模型 评价指标：Null deviance、Residual deviance、AIC。
    模型解释：对于GRE每多考一分，被录取的概率的log值（相对于未被录取）增加0.002；GPA每多考一分，被录取的概率的log值增加0.804；毕业学威望rank是2的（与威望rank是1比较）被录取的概率的log值降低0.675。由于有log的变换，解释会比较奇怪，因此取e变换后即为我们常见的OR值。
    
##内部变量的比较

    运用wald.test（aod包）对rank变量的总效应进行测试。b为模型的系数，Sigma为变量协方差矩阵，Terms为需要比较的内部变量，这里指Rank变量的3个水平，位置4-6。卡方为20.9，p<0.001,说明rank变量有统计学意义。
    
```{r}
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), Terms = 4:6)
```

    假如我们想比较rank=2与rank=3的系数值是否相等，可以用如下操作：
    卡方为5.5，p<0.05,有统计学意义，说明rank=2与rank=3的系数不相等，差异有意义。
```{r}
l <- cbind(0,0,0,1,-1,0)
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), L = l)
```

##模型预测

    我们构建一个新的数据框，来预测下新数据的概率值：
```{r}
newdata1 <- with(mydata,
  data.frame(gre = mean(gre), gpa = mean(gpa), rank = factor(1:4)))

## view data frame
newdata1
```

```{r}
newdata1$rankP <- predict(mylogit, newdata = newdata1, type = "response")
newdata1
```
    
    在以上的结果中，取平均GRE和GPA,我们看到威望rank=1的毕业学校的学生被录取的概率为0.517，而rank=4的学生被录取的概率只有0.185（说明教育背景真的很重要）。
    我们还可以创建一个图表，查看GRE得分与rank在录取概率上的关系。
    
```{r}
newdata2 <- with(mydata,
  data.frame(gre = rep(seq(from = 200, to = 800, length.out = 100), 4),
  gpa = mean(gpa), rank = factor(rep(1:4, each = 100))))
newdata3 <- cbind(newdata2, predict(mylogit, newdata = newdata2, type="link", se=TRUE))
newdata3 <- within(newdata3, {
  PredictedProb <- plogis(fit)
  LL <- plogis(fit - (1.96 * se.fit))
  UL <- plogis(fit + (1.96 * se.fit))
})
head(newdata3)
ggplot(newdata3, aes(x = gre, y = PredictedProb)) +
  geom_ribbon(aes(ymin = LL, ymax = UL, fill = rank), alpha = .2) +
  geom_line(aes(colour = rank), size=1)
```

##模型评价

    最后再来看看模型的拟合好坏，通过summary(mylogit)已经看到了模型评价的指标，现在来做下模型的似然估计测试。结果p<0.001，说明模型拟合有意义。
    
```{r}
with(mylogit, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))
```

```{r}
####模型ROC曲线
pre <- predict(mylogit,data = mydata[,2:4],type = "response")
library(pROC)
modelroc <- roc(mydata$admit,pre,plot = TRUE,print.thres=TRUE, print.auc=TRUE)
plot(modelroc, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1, 0.2),
     grid.col=c("green", "red"), max.auc.polygon=TRUE,
     auc.polygon.col="skyblue", print.thres=TRUE)
```
    
    可以看出，模型的AUC为0.693，在截断点为-0.603时，取值模型敏感性0.744，特异性0.575.
```{r}
library(caret)
#模型评价
pres <- ifelse(pre >= -0.603,1,0)
pres <- as.factor(pres)
mydata$admit <- as.factor(mydata$admit)
xtabs(~pres + mydata$admit)
confusion <- confusionMatrix(pre,mydata$admit)
```

