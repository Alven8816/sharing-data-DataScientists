---
title: "Deep Learning 学习（二）"
author: "余文华"
date: "2016年8月21日"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 学习框架

    卷积神经网络(CNN)的构建有许多开源框架，其中应用比较成熟的有caffe、Mxnet、tensorFlow等。caffe已经有python和matleb接口，在lenux中运行。Mxnet有python、R、scala和Julia接口,windows用户也可方便使用。因此在深度学习领域，完全可以进行黑箱操作，而不需要底层代码。详细内容可百度查阅学习。

###训练步骤：

### 一、数据输入与处理

    首先需要Resize图片，转换存储格式（用caffe开源框架，存储格式为LMDB/LevelDB）。再进行其他处理，3种常用的图像处理方式：去均值；归一化；PCA/白化（图片处理不需要）。进入数据需要是3* n * n结构及1 * n 的label结果。 
    
### 二、定义网络结构(编辑prototxt)

    主要有两种方法，一是自己编辑，模拟搭建一个神经网络结构；二是fine-tuning踩在巨人肩膀上修改一个神经网络。prototxt文件格式源于google的格式化JSON，结构类似于JSON.主要用于图像识别的卷积神经网络“巨人”有AlexNet、VGG、GoogleLeNet、ResNet等，可以在caffe Model Zoo中下载获得。主要做法是复用相同层的权重，新定义层取随机权重初始化，并调大新定义层学习率，调小复用层学习率。
    编辑网络结构主要有以下几种：
  
1. 卷积计算层

    需要设置w和b的学习率lr，deepth(神经元个数)，kernelsize（滑动窗口大小），stride（滑动步长）以及初始化方法及正则化项。在激励层前，可加入Batch Normalization层，使过激励层结果符合高斯分布，并使梯度传递更顺畅。
    
2.  Relu激励层

    主要设置激励函数类型，如RELU等。
    
3. 池化层

    POOLING层主要设置kernel size、stride以及下采样方法：MAX或AVG。
    
4. 全连接层

    主要设置blobs学习率，神经元个数及初始化方法（如“xavier”方法），可加正则化项。

5. 输出层

    最后是SoftMax及Loss Layer，计算得分函数及最后的分类。
    
    
### 三、设置Solver参数(编辑另一个prototxt)

    类似于一个模型的配置文件，类型也是prototxt。主要设置训练迭代轮次（test_iter），经过多少轮测试一次（test_interval），整个模型的基础学习率，及学习衰减率，中途保存和使用GPU还是CPU学习等。
    

### 四、模型训练

    主要设置训练时gpu使用及定位model、solver和权重文件的位置。

### 五、训练检查与监测

    首先，训练网络搭建完成后，需确保网络实现正确，可先把正则化关掉，选择小数据集训练，希望其准确性达到100%，能够“过拟合”；
    其次，加入强度不太大的正则化项，观察loss下降状态。fine-tuning前期loss下降快，中间有瓶颈期。防止过拟合正则化方法除了L1和L2正则化外（深度网络不太试用，因为会增加非常多参数），可以使用Dropout随机失活方法，通过随机设置一些神经元为0，提高模型泛化能力；。
    再次，调整网络结构或学习率，选择最优化方法及参数更新；
    最后，对比训练集和验证集，判断网络优劣。

### 六、图像识别及应用

    最后的图像识别，通过选取1张图片，进入训练好的模型，注意最后的输出层，直接输出prob（概率）。
