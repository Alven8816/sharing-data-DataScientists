---
title: "Deep Learning 学习（一）"
author: "余文华"
date: "2016年8月7日"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 概述

    自从AlphaGo打败李世石后，人类从此就不再淡定了。人工智能、深度学习成了2016年最火爆的字眼。当然，深度学习远远不止在游戏领域挑战着人类的神经，从最早的照相机人脸识别，到微信摇一摇“摇歌曲、摇电视”，再到淘宝的“拍照购”，深度学习的应用其实已经深入到我们生活中的每一个角落。这么神奇的技术，不得不让人垂涎三尺。最近也一直跟着学相关课程，就准备写点学习笔记，学习心得吧！
    深度学习作为机器学习的一种，有其自身的理论体系和框架，自然鄙人远没达到“传道授业解惑也”的水平，感兴趣的朋友希望能系统的去学习，在这里不会介绍太深刻的理论，更多的是个人对一些知识点的理解。
    
##基本流程

    和机器学习一样，拿图像识别为例，深度学习主要包括3步：
    
1. 输入； 通过“喂给”计算机k个类别的N张图片，作为学习的训练集。正如小孩“看图识字”，先给他看各种类型的图片。

2.学习； 让计算机逐次对图片进行“观察”和“学习”。

3.评估； 对学习情况进行评估，通过给计算机一些它不知道类别的图片让它判断，然后再比较对已知正确答案进行修正和提高。计算机“答题”的过程叫做“得分函数”，即对每个类别判断打一个概率“分数”。而衡量图片的判断与正确答案的方法叫“损失函数”，或“代价函数（cost function）”，一般用hinge loss（支持向量机损失）或交叉熵损失（softmax分类器）方法。我们给计算机看图练习的方法进行训练后，一般通过交叉验证进行“测试”。

##神经网络

    人脑之所以充满智慧，是由其特殊的构造决定的。通过无数个神经元进行信号传递和交换，与细胞，触点等一起组成网络，产生生物的意识，帮助生物进行思考和行动。于是乎计算机也创造了一种“人工神经网络”（Artificial Neural Networks，简写为ANNs）,希望模拟大脑的机制，实现人工智能。
    当然一切听起来都是这么的美丽动人，但现实却如此“骨感”，神经网络的提出早在20世纪40年代，但真正有所突破，还是要等待“时机”的到来，随着大型快速计算的逐步成熟，神经网络理论本身以及相关理论、相关技术的不断发展，神经网络的春天才慢慢到来。
    神经网络就是由“输入层”、“隐藏层”和”输出层“构成的网络。隐藏层多的，有多个中间层的即是深度神经网络（DNN）。
    
![image](F:\R\datasciences\Deep Learning\0.jpg)    
    
    多个神经元的组合完成逻辑“或”和“与”的操作和组合就可以对平面样本点进行任意分布点的切割和分类了（至于为什么，推荐大家学习Coursera“机器学习”课程）。
 
![image](F:\R\datasciences\Deep Learning\1.jpg)

    
    神经网络绝不是完美无缺，过犹不及，提升隐藏层数或增加隐藏层中的神经元，会带来过拟合问题，正确的方法应该用正则化或dropout来减缓过拟合。
    
    
##BP神经网络

    前面提到“评估”的过程，通过什么方法来改进“错误”呢。最经典的是BP算法。其基本原理就是“正向传播”求损失，“反向传播”回传误差进行修正，根据误差信号修正每层的权重。像每个考试的学生一样，都希望通过自己的“修正错误”，使自己犯错误最小（误差最小），计算机用到了梯度下降方法，严格来说叫“随机梯度下降（SGD）”，不断进行参数更新，最后得到正确的分类。
    
![image](F:\R\datasciences\Deep Learning\2.jpg)
    
##传递函数

    像人脑对信号的传递一样，神经元对通过的信号数据也会“选择和甄别”，对正向信号给予大的权重，负向信号则“不予通过”，这种评判信号传递强弱的士兵就是“传递函数”。像每一个看门士兵都有自身性格一样，传递函数也主要由sigmod、Tanh、Relu、Maxout等类型来把守传送信号的大门（分别是什么，可自行查阅）。
    
##卷积神经网络

    前面提到的神经网络有个特性叫全连接网络，顾名思义就是每次层神经元都和下一层神经元有权重联系。这在提高识别精确性方面固然有利，但也无疑加大了计算的工作量，特别是输入数据具有大量特征，神经网络层级很多时更是"灾难"。怎么办？聪明的人类发现输入信号的一个特点：“局部关联性”，即输入的信号与其周围信号关联更强，而与远离的关联性更弱。于是就发明了一种“窗口滑动”的办法，每个神经元只对局部数据进行计算，也叫卷积计算层。就像奥运会裁判评判一个跳水运动员的评分一样，以前是每个裁判根据对运动员总体表现的关注打一个分，而现在每个裁判只关注一个局部，有的只关注起跳部分特征，有的只关注落水时的水花特征，这样无疑减轻了裁判的工作量，并且能加快比赛运动员的评分进度。
    卷积神经网络（CNN）拥有更复杂的层级结构，包括数据输入层、卷积计算层、ReLU激励层、池化层和全连接层。输入层和全连接层与一般神经网络一致，ReLU激励层类似于刚刚说到的“传递函数”，这里叫“激励函数”，卷积计算层就是上面讲到的“通过窗口滑动，每个神经元只对局部数据进行计算”。前面的层级主要从减少参数（权重）的角度入手，降低机器计算量，而池化层，简单理解就是通过放缩，类似“降低像素分辨率”方法，降低输入数据本身来减少计算。而其他的训练算法与BP神经网络是一致的。
    最后谈谈fine-tuning方法，即“站在巨人肩膀上”，使用已经训练好的模型权重，作为初始值开始训练来构建符合自身数据特点的卷积神经网络结构，比如AlexNet、GoogLeNet、VGGNet等典型的CNN框架结构。
    

    



