)
Affiliation(EUtilsGet(res2))
library(ggplot2)
y <- YearPubmed(EUtilsGet(res))
table(y)
res <- EUtilsSummary("Meta analysis[ti]",type = "esearch",db="pubmed",mindate= 2000,maxdate=2015)
summary(res)
eachyear<- array()
x <- 1
for (i in 2000:2015){
Sys.sleep(1)
r <- EUtilsSummary('Meta analysis[ti]', type='esearch', db='pubmed', mindate=i, maxdate=i)
eachyear[x] <- QueryCount(r)
x <- x + 1
}
table(eachyear)
date()
count<-table(eachyear)
count
count<-as.data.frame(count)
count
names(count)<-c("Year", "Counts")
count
names(eachyear) <- 2000:2015
names(eachyear) <- 2000:2015
eachyear
count<-table(eachyear)
count
count<-as.data.frame(count)
count
count<-as.data.frame(eachyear)
count
names(count)<-c("Year", "Counts")
eachyear
data.frame(eachyear)
year <- row.names(eachyear)
year
year<- 2000:2015
eachyear<- array()
x <- 1
for (i in 2000:2015){
Sys.sleep(1)
r <- EUtilsSummary('Meta analysis[ti]', type='esearch', db='pubmed', mindate=i, maxdate=i)
eachyear[x] <- QueryCount(r)
x <- x + 1
}
count<-data.frame(year,eachyear)
View(count)
names(count)<-c("Year", "Counts")
num <- data.frame(Year=count$Year, Counts=cumsum(count$Counts))
num
num$g <- "g"
names(num) <- c("Year", "Counts", "g")
num
q <- qplot(x=Year, y=Counts, data=count, geom="bar", stat="identity")
q <- q + ggtitle(paste("PubMed articles containing \'", g,            "\' ", "= ", max(num$Counts), sep="")) +
ylab("Number of articles") +
xlab(paste("Year \n Query date: ", Sys.time(), sep="")) +
labs(colour="") +
theme_bw()
q <- qplot(x=Year, y=Counts, data=count, geom="bar", stat="identity")
q <- q + ggtitle(Number of PubMed articles \n which its title containing Meta analysis) +
ylab("Number of articles") +
xlab(paste("Year \n Query date: ", Sys.time(), sep="")) +
labs(colour="") +
theme_bw()
q
year<- 2000:2015
max(eachyear)
count<-data.frame(year,eachyear)
barplot(eachyear, las=2, ylim=c(0,10000), main="Number of PubMed articles \n which its title containing Meta analysis")
affs<- array()
outcome <- array()
x <- 1
for (i in 2000:2015){
Sys.sleep(1)
r <- EUtilsSummary('Meta analysis[ti]', type='esearch', db='pubmed', mindate=i, maxdate=i)
affs[x] <- ifelse("china" %in% Affiliation(EUtilsGet(r)) , 1,0)
outcome[x] <- sum(affs[x])
x <- x + 1
}
outcome
affs
aff <- Affiliation(EUtilsGet(res2))
res2 <- EUtilsSummary("Wenhua Yu",type = "esearch",db = "pubmed",
mindate = "2014",maxdate = "2014")
aff <- Affiliation(EUtilsGet(res2))
ifelse("china" %in% aff,1,0)
"china" %in% aff
aff
match("china",aff)
match("china",aff[1])
"china" %in% aff[1]
aff[1]
"China" %in% aff[1]
"China." %in% aff[1]
"of" %in% aff[1]
"Chinese" %in% aff[1]
aff
gregexpr("China",aff[1])
grepl("China",aff[1])
grepl("China",aff)
grep("China",aff)
grepl("China",aff)
sum(grepl("China",aff))
ifelse(grepl("China",aff),1,0)
affs<- array()
x <- 1
for (i in 2000:2015){
Sys.sleep(1)
r <- EUtilsSummary('Meta analysis[ti]', type='esearch', db='pubmed', mindate=i, maxdate=i)
affs[x] <- sum(grepl("China",Affiliation(EUtilsGet(r))))
x <- x + 1
}
affs
data<-data.frame(year,eachyear,china=affs)
data
barplot(data, las=2, ylim=c(0,10000), main="Number of PubMed articles \n which its title containing Meta analysis")
barplot(data[2,3], las=2, ylim=c(0,10000), main="Number of PubMed articles \n which its title containing Meta analysis")
barplot(data[3], las=2, ylim=c(0,500), main="Number of PubMed articles \n which its title containing Meta analysis")
datas<-data.frame(year,eachyear,china=affs)
class(datas)
class(datas$year)
class(datas$eachyear)
class(datas$china)
barplot(as.list(datas[,2:3]), las=2, ylim=c(0,500), main="Number of PubMed articles \n which its title containing Meta analysis")
datas<-data.frame(year,eachyear,china=as.numeric(affs))
datas
pro <- datas$china/datas$eachyear*100
pro
pro <-affs/eachyear*100
datas<-data.frame(year,eachyear,china=as.numeric(affs),pro)
datas
barplot(as.list(datas[,2:3]), las=2, ylim=c(0,1000), main="Number of PubMed articles \n which its title containing Meta analysis")
barplot(as.list(eachyear,affs), las=2, ylim=c(0,1000), main="Number of PubMed articles \n which its title containing Meta analysis")
barplot(count, las=2, ylim=c(0,10000), main="Number of PubMed articles \n which its title containing Meta analysis")
barplot(count$eachyear, las=2, ylim=c(0,10000), main="Number of PubMed articles \n which its title containing Meta analysis")
barplot(pro, las=2, ylim=c(0,100), main="Number of PubMed articles \n which its title containing Meta analysis")
barplot(pro, las=2, ylim=c(0,10), main="Number of PubMed articles \n which its title containing Meta analysis")
barplot(count$eachyear, las=2, ylim=c(0,10000), main="Number of PubMed articles \n which its title containing Meta analysis")
axis(1,seq(from = 0.7,by = 1.2, length.out = 16),labels = year,tick = FALSE,cex.axis = 0.8)
barplot(count$eachyear, las=2, ylim=c(0,10000), main="Number of PubMed articles \n which its title containing Meta analysis")
axis(1,seq(from = 0.7,by = 1.2, length.out = 16),labels = year,tick = FALSE,cex.axis = 0.8)
text(labels = eachyear,cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= eachyear+10)
barplot(count$eachyear, las=2, ylim=c(0,10000), main="Number of PubMed articles \n which its title containing Meta analysis")
axis(1,seq(from = 0.7,by = 1.2, length.out = 16),labels = year,tick = FALSE,cex.axis = 0.8)
text(labels = eachyear,cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= eachyear+100)
barplot(count$eachyear, las=2, ylim=c(0,10000), main="Number of PubMed articles \n which its title containing Meta analysis")
axis(1,seq(from = 0.7,by = 1.2, length.out = 16),labels = year,tick = FALSE,cex.axis = 0.8)
text(labels = eachyear,cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= eachyear+1000)
barplot(count$eachyear, las=2, ylim=c(0,12000), main="Number of PubMed articles \n which its title containing Meta analysis")
axis(1,seq(from = 0.7,by = 1.2, length.out = 16),labels = year,tick = FALSE,cex.axis = 0.8)
text(labels = eachyear,cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= eachyear+1000)
barplot(count$eachyear, las=2, ylim=c(0,11000), main="Number of PubMed articles \n which its title containing Meta analysis")
axis(1,seq(from = 0.7,by = 1.2, length.out = 16),labels = year,tick = FALSE,cex.axis = 0.8)
text(labels = eachyear,cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= eachyear+1000)
barplot(count$eachyear, las=2, ylim=c(0,11000), main="Number of PubMed articles \n which its title containing Meta analysis")
axis(1,seq(from = 0.7,by = 1, length.out = 16),labels = year,tick = FALSE,cex.axis = 0.8)
text(labels = eachyear,cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= eachyear+1000)
barplot(count$eachyear, las=2, ylim=c(0,11000), main="Number of PubMed articles \n which its title containing Meta analysis")
axis(1,seq(from = 0.7,by = 1.2, length.out = 16),labels = year,tick = FALSE,cex.axis = 0.8)
text(labels = eachyear,cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= eachyear+1000)
barplot(pro, las=2, ylim=c(0,10), main="Percentage of Meta analysis which come from China")
axis(1,seq(from = 0.7,by = 1.2, length.out = 16),labels = year,tick = FALSE,cex.axis = 0.8)
text(labels = paste(round(pro,2),"%",sep = ""),cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= pro+0.2)
barplot(pro, las=2, ylim=c(0,10), main="Percentage of Meta analysis which come from China")
axis(1,seq(from = 0.7,by = 1.2, length.out = 16),labels = year,tick = FALSE,cex.axis = 0.8)
text(labels = paste(round(pro,2),"%",sep = ""),cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= pro+0.3)
barplot(pro, las=2, ylim=c(0,10), main="Percentage of Meta analysis which come from China")
axis(1,seq(from = 0.7,by = 1.2, length.out = 16),labels = year,tick = FALSE,cex.axis = 0.8)
text(labels = paste(round(pro,2),"%",sep = ""),cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= pro+0.4)
barplot(pro, las=2, ylim=c(0,10), main="Percentage of Meta analysis which come from China")
axis(1,seq(from = 0.7,by = 1.2, length.out = 16),labels = year,tick = FALSE,cex.axis = 0.8)
text(labels = paste(round(pro,2),"%",sep = ""),cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= pro+0.5)
barplot(pro, las=2, ylim=c(0,10),ylab = "%", main="Percentage of Meta analysis which come from China")
axis(1,seq(from = 0.7,by = 1.2, length.out = 16),labels = year,tick = FALSE,cex.axis = 0.8)
text(labels = paste(round(pro,2),"%",sep = ""),cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= pro+0.5)
barplot(pro, las=2, ylim=c(0,10),ylab = "percentage", main="Percentage of Meta analysis which come from China")
barplot(pro, las=2, ylim=c(0,10),ylab = "percentage(%)", main="Percentage of Meta analysis which come from China")
axis(1,seq(from = 0.7,by = 1.2, length.out = 16),labels = year,tick = FALSE,cex.axis = 0.8)
text(labels = paste(round(pro,2),"%",sep = ""),cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= pro+0.5)
affs
table(datas)
names(affs)<- 2000:2015
affs
table(affs)
barplot(count$eachyear, las=2, ylim=c(0,11000), main="Number of PubMed articles \n which its title containing Meta analysis")
axis(1,seq(from = 0.7,by = 1.2, length.out = 16),labels = year,tick = FALSE,cex.axis = 0.8)
text(labels = eachyear,cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= eachyear+1000)
barplot(pro, las=2, ylim=c(0,10),ylab = "percentage(%)", main="Percentage of Meta analysis which come from China")
axis(1,seq(from = 0.7,by = 1.2, length.out = 16),labels = year,tick = FALSE,cex.axis = 0.8)
text(labels = paste(round(pro,2),"%",sep = ""),cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= pro+0.5)
library(wordcloud)
library(wordcloud)
res2 <- EUtilsSummary("Meta analysis[ti]",type = "esearch",db="pubmed",reldate=30)
EUtilsGet(res2)
QueryCount(EUtilsGet(res2))
Author(EUtilsGet(res2))
QueryCount(EUtilsGet(res2))
QueryId(res2)
res2 <- EUtilsSummary("Meta analysis[ti]",type = "esearch",db="pubmed",reldate=7)
QueryId(res2)
res2 <- EUtilsSummary("Wenhua Yu[au]",type = "esearch",db="pubmed",mindate= 2013,maxdate=2015)
summary(res2)
Affiliation(EUtilsGet(res2))
grepl("Tianjin",Affiliation(EUtilsGet(r)))
grepl("Tianjin",Affiliation(EUtilsGet(res2)))
index <- grepl("Tianjin",Affiliation(EUtilsGet(res2)))
res2[index]
ArticleId(EUtilsGet(res2))
res2 <- EUtilsSummary("Wenhua Yu[au] Tianjin",type = "esearch",db="pubmed",mindate= 2013,maxdate=2015)
Summary(res2)
summary(res2)
Title(EUtilsGet(res2))
ArticleTitle(EUtilsGet(res2))
library(wordcloud)
Abstract<- AbstractText(EUtilsGet(res2))
articles<-data.frame(Abstract))
articles<-data.frame(Abstract)
abstracts<-as.character(articles$Abstract)
abstracts<-paste(abstracts, sep="", collapse="")
wordcloud(abstracts, min.freq=10, max.words=70, colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts, min.freq=10, max.words=500, colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts, min.freq=10, max.words=1000, colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts, min.freq=5, max.words=1000, colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts, min.freq=3, max.words=1000, colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts, min.freq=4, max.words=500, colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts, min.freq=5, max.words=500, colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts, min.freq=8, max.words=500, colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts, min.freq=8, max.words=1000, colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts, min.freq=7, max.words=1000, colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts, min.freq=6, max.words=1000, colors=brewer.pal(7,"Dark2")
wordcloud(abstracts, min.freq=6, max.words=1000, colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts, min.freq=5, max.words=1000, colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts, min.freq=5, max.words=500, colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts, min.freq=5, max.words=1000, colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts, min.freq=4, max.words=1000, colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts, min.freq=4, max.words=500, colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts, min.freq=3, max.words=500, colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts, min.freq=3, max.words=1000, colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts, min.freq=3, max.words=500, colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts, min.freq=4, max.words=500, colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts, min.freq=3, max.words=500, colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts, colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts,max.words=500,
min.freq = 10 colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts,max.words=500,
min.freq = 10,colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts,max.words=500,
min.freq = 5,colors=brewer.pal(7,"Dark2"))
wordcloud(abstracts,colors=brewer.pal(7,"Dark2"))
sum(eachyear)
affs
library(RISmed)
barplot(pro, las=2, ylim=c(0,10),ylab = "percentage(%)", main="Percentage of Meta analysis which come from China")
axis(1,seq(from = 0.7,by = 1.2, length.out = 16),labels = year,tick = FALSE,cex.axis = 0.8)
text(labels = paste(round(pro,2),"%",sep = ""),cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= pro+0.5)
barplot(affs, las=2, ylim=c(0,500), main="Number of Meta analysis articles \n in PubMed which come from China ")
axis(1,seq(from = 0.7,by = 1.2, length.out = 16),labels = year,tick = FALSE,cex.axis = 0.8)
text(labels = eachyear,cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= affs+50)
barplot(affs, las=2, ylim=c(0,500), main="Number of Meta analysis articles \n in PubMed which come from China ")
text(labels = eachyear,cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= affs+100)
barplot(affs, las=2, ylim=c(0,500), main="Number of Meta analysis articles \n in PubMed which come from China ")
text(labels = eachyear,cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= affs+50)
barplot(affs, las=2, ylim=c(0,500), main="Number of Meta analysis articles \n in PubMed which come from China ")
text(labels = affs,cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= affs+50)
barplot(count$eachyear, las=2, ylim=c(0,11000), main="Number of PubMed articles \n which its title containing Meta analysis")
text(labels = eachyear,cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= eachyear+1000)
barplot(pro, las=2, ylim=c(0,10),ylab = "percentage(%)", main="Percentage of Meta analysis which come from China")
axis(1,seq(from = 0.7,by = 1.2, length.out = 16),labels = year,tick = FALSE,cex.axis = 0.8)
text(labels = paste(round(pro,2),"%",sep = ""),cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= pro+0.5)
barplot(pro, las=2, ylim=c(0,10),ylab = "percentage(%)", main="Percentage of Meta analysis \n which come from China")
axis(1,seq(from = 0.7,by = 1.2, length.out = 16),labels = year,tick = FALSE,cex.axis = 0.8)
text(labels = paste(round(pro,2),"%",sep = ""),cex = 0.7,x = seq(from = 0.7,by =1.2,length.out = 16),y= pro+0.5)
hcc <- c(166,47)
hbv <- c(129,73)
data <- data.frame(hcc,hbv,row.names = c("male","female"))
data
table(data)
fisher.test(data)
chisq.test(data)
install.packages("Rweibo", repos = "http://jliblog.com/cran")
install.packages("twitteR")
install.packages("ROAuth")
library(twitteR)
library(ROAuth)
require(RCurl)
library(stringr)
library(tm)
library(ggmap)
library(dplyr)
library(plyr)
library(tm)
library(wordcloud)
library(wordcloud)
library(twitteR)
library(ROAuth)
require(RCurl)
library(stringr)
library(tm)
library(ggmap)
library(dplyr)
library(plyr)
library(tm)
library(wordcloud)
setwd("C:/Users/HP/Desktop/乐享数据资料包/Sentiment Analysis on Donald Trump using R and Tableau")
key="hidden"
secret="hidden"
download.file(url="http://curl.haxx.se/ca/cacert.pem",
destfile="/text_mining_and_web_scraping/cacert.pem",
method="auto")
download.file(url="http://curl.haxx.se/ca/cacert.pem",
destfile="/text_mining_and_web_scraping/cacert.pem",
method="auto")
download.file(url="http://curl.haxx.se/ca/cacert.pem",
method="auto")
download.file(url="http://curl.haxx.se/ca/cacert.pem",destfile = "cacert.pm",method="auto")
authenticate <- OAuthFactory$new(consumerKey=key,
consumerSecret=secret,
requestURL="https://api.twitter.com/oauth/request_token",
accessURL="https://api.twitter.com/oauth/access_token",
authURL="https://api.twitter.com/oauth/authorize")
setup_twitter_oauth(key, secret)
setup_twitter_oauth(key, secret)
N=2000  # tweets to request from each query
S=200  # radius in miles
lats=c(38.9,40.7,37.8,39,37.4,28,30,42.4,48,36,32.3,33.5,34.7,33.8,37.2,41.2,46.8,
46.6,37.2,43,42.7,40.8,36.2,38.6,35.8,40.3,43.6,40.8,44.9,44.9)
lons=c(-77,-74,-122,-105.5,-122,-82.5,-98,-71,-122,-115,-86.3,-112,-92.3,-84.4,-93.3,
-104.8,-100.8,-112, -93.3,-89,-84.5,-111.8,-86.8,-92.2,-78.6,-76.8,-116.2,-98.7,-123,-93)
#cities=DC,New York,San Fransisco,Colorado,Mountainview,Tampa,Austin,Boston,
#       Seatle,Vegas,Montgomery,Phoenix,Little Rock,Atlanta,Springfield,
#       Cheyenne,Bisruk,Helena,Springfield,Madison,Lansing,Salt Lake City,Nashville
#       Jefferson City,Raleigh,Harrisburg,Boise,Lincoln,Salem,St. Paul
donald=do.call(rbind,lapply(1:length(lats), function(i) searchTwitter('Donald+Trump',
lang="en",n=N,resultType="recent",
geocode=paste(lats[i],lons[i],paste0(S,"mi"),sep=","))))
install.packages("curl")
N=2000  # tweets to request from each query
S=200  # radius in miles
lats=c(38.9,40.7,37.8,39,37.4,28,30,42.4,48,36,32.3,33.5,34.7,33.8,37.2,41.2,46.8,
46.6,37.2,43,42.7,40.8,36.2,38.6,35.8,40.3,43.6,40.8,44.9,44.9)
lons=c(-77,-74,-122,-105.5,-122,-82.5,-98,-71,-122,-115,-86.3,-112,-92.3,-84.4,-93.3,
-104.8,-100.8,-112, -93.3,-89,-84.5,-111.8,-86.8,-92.2,-78.6,-76.8,-116.2,-98.7,-123,-93)
donald=do.call(rbind,lapply(1:length(lats), function(i) searchTwitter('Donald+Trump',
lang="en",n=N,resultType="recent",
geocode=paste(lats[i],lons[i],paste0(S,"mi"),sep=","))))
donaldlat=sapply(donald, function(x) as.numeric(x$getLatitude()))
donaldlat=sapply(donaldlat, function(z) ifelse(length(z)==0,NA,z))
data <- data.frame(tweet = "I LOVE MY COUNTRY AND FAMILITY, I LOVE MY LIFE TOO. hollow world , hollow R laungrage , hollo sweet and love my girlfrend")
View(data)
VectorSource(data$tweet)
Corpus(VectorSource(data$tweet))
corpus=Corpus(VectorSource(data$tweet))
data("crude")
crude
tm_map(crude, stemDocument, lazy = TRUE)[[1]]
corpus=tm_map(corpus,tolower)
tm_map(corpus,function(x) removeWords(x,stopwords()))
corpus=tm_map(corpus,function(x) removeWords(x,stopwords()))
corpus=tm_map(corpus,PlainTextDocument)
# convert corpus to a Plain Text Document
corpus
meta(corpus)
TextDocument(corpus)
col=brewer.pal(6,"Dark2")
wordcloud(corpus, min.freq=25, scale=c(5,2),rot.per = 0.25,
random.color=T, max.word=45, random.order=F,colors=col)
wordcloud(corpus, min.freq=1, scale=c(5,2),rot.per = 0.25,
random.color=T, max.word=45, random.order=F,colors=col)
wordcloud(corpus, min.freq=25, scale=c(4,0.5),rot.per = 0.25,
random.color=T, max.word=45, random.order=F,colors=col)
col=brewer.pal(6,"Dark2")
wordcloud(corpus, min.freq=25, scale=c(5,2),rot.per = 0.25,
random.color=T, max.word=45, random.order=F,colors=col)
data$lat <- c(38.9,40.7,37.8,39,37.4,28,30,42.4,48,36,32.3,33.5,34.7,33.8,37.2,41.2,46.8,
46.6,37.2,43,42.7,40.8,36.2,38.6,35.8,40.3,43.6,40.8,44.9,44.9)
data<- NULL
data$lat <- c(38.9,40.7,37.8,39,37.4,28,30,42.4,48,36,32.3,33.5,34.7,33.8,37.2,41.2,46.8,
46.6,37.2,43,42.7,40.8,36.2,38.6,35.8,40.3,43.6,40.8,44.9,44.9)
data$lon <- c(-77,-74,-122,-105.5,-122,-82.5,-98,-71,-122,-115,-86.3,-112,-92.3,-84.4,-93.3,
-104.8,-100.8,-112, -93.3,-89,-84.5,-111.8,-86.8,-92.2,-78.6,-76.8,-116.2,-98.7,-123,-93)
head(data)
data<- data.frame()
data$lat <- c(38.9,40.7,37.8,39,NA,28,30,42.4,48,36,32.3,33.5,34.7,33.8,37.2,41.2,46.8,
46.6,NA,43,42.7,40.8,36.2,38.6,35.8,40.3,43.6,40.8,44.9,44.9)
data <- data.frame(lat <- c(38.9,40.7,37.8,39,NA,28,30,42.4,48,36,32.3,33.5,34.7,33.8,37.2,41.2,46.8,
+               46.6,NA,43,42.7,40.8,36.2,38.6,35.8,40.3,43.6,40.8,44.9,44.9),lons=c(NA,-74,-122,-105.5,-122,-82.5,-98,-71,-122,-115,-86.3,-112,-92.3,-84.4,-93.3,
NA,-100.8,-112, -93.3,-89,-84.5,-111.8,-86.8,-92.2,-78.6,-76.8,-116.2,-98.7,-123,-93))
View(data)
View(data)
names(data)
names(data)<- c("lat","lons")
names(data)
data=filter(data, !is.na(lat),!is.na(lons))
data
lonlat=select(data,lon,lat)
lonlat=select(data,lons,lat)
lonlat
result <- do.call(rbind, lapply(1:nrow(lonlat),
function(i) revgeocode(as.numeric(lonlat[i,1:2]))))
positives= readLines("positivewords.txt")
positives= readLines("positive-words.txt")
str(positives)
negatives = readLines("negative-words.txt")
positives[100:200]
sentiment_scores = function(tweets, positive_words, negative_words, .progress='none'){
scores = laply(tweets,
function(tweet, positive_words, negative_words){
tweet = gsub("[[:punct:]]", "", tweet)    # remove punctuation
tweet = gsub("[[:cntrl:]]", "", tweet)   # remove control characters
tweet = gsub('\\d+', '', tweet)          # remove digits
# Let's have error handling function when trying tolower
tryTolower = function(x){
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
# result
return(y)
}
# use tryTolower with sapply
tweet = sapply(tweet, tryTolower)
# split sentence into words with str_split function from stringr package
word_list = str_split(tweet, "\\s+")
words = unlist(word_list)
# compare words to the dictionaries of positive & negative terms
positive_matches = match(words, positive_words)
negative_matches = match(words, negative_words)
# get the position of the matched term or NA
# we just want a TRUE/FALSE
positive_matches = !is.na(positive_matches)
negative_matches = !is.na(negative_matches)
# final score
score = sum(positive_matches) - sum(negative_matches)
return(score)
}, positive_matches, negative_matches, .progress=.progress )
return(scores)
}
tweet <- c("I love my country but i alse hate eatting hamburger","I like my family too much","the world is bad but it is soo pretty and sweet")
score = sentiment_scores(tweet, positives, negatives, .progress='text')
positives= readLines("positive-words.txt")
negatives = readLines("negative-words.txt")
score = sentiment_scores(tweet, positives, negatives, .progress='text')
str_split(tweet, "\\s+")
tweet = sapply(tweet, tryTolower)
tryTolower = function(x){
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
# result
return(y)
}
tweet = sapply(tweet, tryTolower)
str_split(tweet, "\\s+")
word_list = str_split(tweet, "\\s+")
words = unlist(word_list)
words
match(words, positive_words)
match(words, positives)
a <-match(words, positives)
!is.na(a)
sum(!is.na(a))
sentiment_scores = function(tweets, positive_words, negative_words, .progress='none'){
scores = laply(tweets,
function(tweet, positive_words, negative_words){
tweet = gsub("[[:punct:]]", "", tweet)    # remove punctuation
tweet = gsub("[[:cntrl:]]", "", tweet)   # remove control characters
tweet = gsub('\\d+', '', tweet)          # remove digits
# Let's have error handling function when trying tolower
tryTolower = function(x){
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
# result
return(y)
}
# use tryTolower with sapply
tweet = sapply(tweet, tryTolower)
# split sentence into words with str_split function from stringr package
word_list = str_split(tweet, "\\s+")
words = unlist(word_list)
# compare words to the dictionaries of positive & negative terms
positive_matches = match(words, positive_words)
negative_matches = match(words, negative_words)
# get the position of the matched term or NA
# we just want a TRUE/FALSE
positive_matches = !is.na(positive_matches)
negative_matches = !is.na(negative_matches)
# final score
score = sum(positive_matches) - sum(negative_matches)
return(score)
}, positive_matches, negative_matches, .progress=.progress )
return(scores)
}
sentiment_scores(tweets = tweet,positive_words = positives,negative_words = negatives,.progress = "text")
positive_matches <- NULL
negative_matches <- NULL
sentiment_scores(tweet, positives, negatives, .progress='text')
tweet <- "i love you too much "
sentiment_scores(tweet, positives, negatives, .progress='text')
