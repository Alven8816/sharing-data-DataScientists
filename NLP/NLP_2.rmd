---
title: "NLP_2"
author: "余文华"
date: "2017年1月15日"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##数据导入生成语料库

```{r}
#取得语料
library(tm)
##1.Data Import  导入自带的路透社的20篇xml文档  
#找到/texts/crude的目录，作为DirSource的输入，读取20篇xml文档  
reut21578 <- system.file("texts", "crude", package = "tm")  
reuters <- Corpus(DirSource(reut21578), readerControl = list(reader = readReut21578XML))  
#Corpus命令读取文本并生成语料库文件 
#保存为本地文件
writeCorpus(reuters)
#Inspecting Corpora查看语料
inspect(reuters)
print(reuters)
summary(reuters)
#转存为行为单位的字符串
writeLines(as.character(reuters[[2]]))
lapply(reuters[1:2], as.character)
```

##文本转换

```{r}
#去除标签,转换为纯文本
reuters <- tm_map(reuters,PlainTextDocument)
#删除空白
reuters <- tm_map(reuters,stripWhitespace)
#转换为小写
reuters <- tm_map(reuters, content_transformer(tolower))
#去掉停词
reuters <- tm_map(reuters,removeWords,stopwords('english'))
#词干提取
crude <- tm_map(reuters,stemDocument)
```

##Metadata 管理

```{r}
DublinCore(crude[[1]],"Creator") <- "Ano Nymos"
meta(crude)

meta(crude,tag = "test",type = "corpus") <- "test meta"
meta(crude,type = "corpus")
```

##创建文档-词频矩阵

```{r}
dtm <- DocumentTermMatrix(reuters)
#查看文档
inspect(dtm[1:5, 100:105]) 

#Non-/sparse entries: 1990/22390     ---非0/是0   
#Sparsity           : 92%            ---稀疏性  稀疏元素占全部元素的比例  
#Maximal term length: 17             ---切词结果的字符最长那个的长度  
#Weighting          : term frequency (tf)---词频率  
#如果需要考察多个文档中特有词汇的出现频率，可以手工生成字典，并将它作为生成矩阵的参数
d<-c("price","crude","oil","use")   #以这几个关键词为查询工具  
inspect(DocumentTermMatrix(reuters,control=list(dictionary=d)))  
```
##在文档词频矩阵上操作

```{r}
##6. Operations on Term-Document Matrices  
#找出次数超过50的词  
findFreqTerms(dtm, 50)  
#找出与‘opec’单词相关系数在0.8以上的词  
findAssocs(dtm,"opec",0.8)

#因为生成的矩阵是一个稀疏矩阵，再进行降维处理，之后转为标准数据框格式  
#我们可以去掉某些出现频次太低的词。
dtm1<- removeSparseTerms(dtm, sparse=0.6)  
inspect(dtm1)  
data <- as.data.frame(inspect(dtm1)) 
row.names(data)<- 1:20
```

##主题分析
```{r}
library(topicmodels)
ctm <- CTM(dtm, k = 2)
terms(ctm, 2, 0.05)
```

##聚类分析
```{r}
#再之后就可以利用R语言中任何工具加以研究了，下面用层次聚类试试看  
#先进行标准化处理，再生成距离矩阵，再用层次聚类  
data.scale <- scale(data)  
d <- dist(data.scale, method = "euclidean")  
fit <- hclust(d, method="ward.D")  
  
  
#绘制聚类图  
#可以看到在20个文档中，489号和502号聚成一类，与其它文档区别较大。  
plot(fit,main ="文件聚类分析",hang = -1)  
```

##补充
```{r}
##词干化
stemDocument(c('functions', 'stemming', 'liked', 'doing'))
#"function" "stem"     "like"     "do"  
##记号化
MC_tokenizer('我爱你')
scan_tokenizer('I love you')
data("crude")
MC_tokenizer(crude[[1]])
scan_tokenizer(crude[[1]])
```


## 参考资料

>- Rwordseg、Rweibo、tm的安装<http://www.dataguru.cn/thread-482875-1-1.html>

>- R语言与NLP <http://blog.csdn.net/column/details/13670.html>

>- 利用word2vec对关键词进行聚类<http://blog.csdn.net/zhaoxinfan/article/details/11069485>

>- 深度学习 word2vec 笔记<http://www.open-open.com/lib/view/1420689477515#articleHeader0>

